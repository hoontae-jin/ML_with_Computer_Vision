{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/EzIKsJQzuDTgU3QIBfgh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_tPJ7fRhBHY","executionInfo":{"status":"ok","timestamp":1669385776624,"user_tz":-540,"elapsed":29324,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}},"outputId":"c7b7f015-933e-48c7-db5c-88d19011fd17"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-11-25 14:15:46--  https://www.dropbox.com/s/g6b6gtvmdu0h77x/ShoeV2_photo.zip\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6031:18::a27d:5112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/raw/g6b6gtvmdu0h77x/ShoeV2_photo.zip [following]\n","--2022-11-25 14:15:47--  https://www.dropbox.com/s/raw/g6b6gtvmdu0h77x/ShoeV2_photo.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com/cd/0/inline/BxYvBq2878vyjnuYlKl930rshSIVyblMwwrhFFXdnIMrX52cI9m403RE12gVpZl-P_5l9wz8QysRu8Y_4fFOwcyVRPZOk6ColIcAt24s_gD8V5g-z1nYZjQYbj0n5oEUEuqMH9rcX3EAeVOqaD01XX8ZK4NRdVVSFbOyQyMmd-ElKA/file# [following]\n","--2022-11-25 14:15:48--  https://uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com/cd/0/inline/BxYvBq2878vyjnuYlKl930rshSIVyblMwwrhFFXdnIMrX52cI9m403RE12gVpZl-P_5l9wz8QysRu8Y_4fFOwcyVRPZOk6ColIcAt24s_gD8V5g-z1nYZjQYbj0n5oEUEuqMH9rcX3EAeVOqaD01XX8ZK4NRdVVSFbOyQyMmd-ElKA/file\n","Resolving uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com (uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n","Connecting to uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com (uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BxaKQgCvwBFpC9yFKlFy7GkMKQSk_zvNF9dMU3GBt0km3BJuSHzPw77DjgCb6rPC_F1J_IEgfO-70K-JWcE5W4NYbErCUGrFZKODgxlpsRiFkgikP_FbkK_jk-xsYyBj_UkMGpzsRMRACATqXWEDmZ82S2zdixxBV2DRT9u8Ph2Hytzu3hTBYKhH3_5-_Cdupm2UEijcBEahsyCybu8r3wurBRQMKlLgomXxJOqBlMZHmU0P8MjuKCk_NBiEHDPDzbrj_SH2mMhNLz6t3kfAmpIuEyhgOLGWHotJsD9RtvPx-_Q6PuhRT9iMsQ4OX2IDsZ152ONPDpDkSbghC_YhUYBKYLBtRR2VaJKN42Y47e92jiMLPlh6bSLuyhBIUVF7vBqAg8JrVeKH5knjV-HRBk6kuglLxEc5afbVKEhnt0YVNA/file [following]\n","--2022-11-25 14:15:49--  https://uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com/cd/0/inline2/BxaKQgCvwBFpC9yFKlFy7GkMKQSk_zvNF9dMU3GBt0km3BJuSHzPw77DjgCb6rPC_F1J_IEgfO-70K-JWcE5W4NYbErCUGrFZKODgxlpsRiFkgikP_FbkK_jk-xsYyBj_UkMGpzsRMRACATqXWEDmZ82S2zdixxBV2DRT9u8Ph2Hytzu3hTBYKhH3_5-_Cdupm2UEijcBEahsyCybu8r3wurBRQMKlLgomXxJOqBlMZHmU0P8MjuKCk_NBiEHDPDzbrj_SH2mMhNLz6t3kfAmpIuEyhgOLGWHotJsD9RtvPx-_Q6PuhRT9iMsQ4OX2IDsZ152ONPDpDkSbghC_YhUYBKYLBtRR2VaJKN42Y47e92jiMLPlh6bSLuyhBIUVF7vBqAg8JrVeKH5knjV-HRBk6kuglLxEc5afbVKEhnt0YVNA/file\n","Reusing existing connection to uc3a362d19195e2744b697877c23.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 257637580 (246M) [application/zip]\n","Saving to: ‘ShoeV2_photo.zip’\n","\n","ShoeV2_photo.zip    100%[===================>] 245.70M  10.2MB/s    in 23s     \n","\n","2022-11-25 14:16:12 (10.7 MB/s) - ‘ShoeV2_photo.zip’ saved [257637580/257637580]\n","\n"]}],"source":["import os\n","if not os.path.exists('ShoeV2_photo'):\n","    !wget https://www.dropbox.com/s/g6b6gtvmdu0h77x/ShoeV2_photo.zip\n","    !unzip -q ShoeV2_photo.zip"]},{"cell_type":"code","source":["!pip install torch_snippets\n","from torch_snippets import *\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nQvWp72chu_e","executionInfo":{"status":"ok","timestamp":1669385797318,"user_tz":-540,"elapsed":20697,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}},"outputId":"bc61de14-0755-45ed-d88e-3d8bb1651bfa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_snippets\n","  Downloading torch_snippets-0.499.9-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (7.9.0)\n","Collecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (1.3.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (6.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (7.1.2)\n","Requirement already satisfied: fastcore in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (1.5.27)\n","Collecting jsonlines\n","  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (0.3.6)\n","Collecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (1.21.6)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (0.4.0)\n","Requirement already satisfied: confection in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (0.0.3)\n","Requirement already satisfied: srsly in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (2.4.5)\n","Requirement already satisfied: altair in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (4.2.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (3.2.2)\n","Requirement already satisfied: catalogue in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (2.0.8)\n","Collecting python-Levenshtein\n","  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n","Collecting xmltodict\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Requirement already satisfied: wasabi in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (0.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (4.64.1)\n","Collecting loguru\n","  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (1.10.2)\n","Collecting rich\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[K     |████████████████████████████████| 237 kB 68.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (4.1.1)\n","Collecting typing\n","  Downloading typing-3.7.4.3.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 9.3 MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from torch_snippets) (3.7)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.8.5.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.7.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.15.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->torch_snippets) (4.6.0.66)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->torch_snippets) (0.18.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->torch_snippets) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (1.3.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2.6.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch_snippets) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch_snippets) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch_snippets) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch_snippets) (3.0.9)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair->torch_snippets) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair->torch_snippets) (2.11.3)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair->torch_snippets) (4.3.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair->torch_snippets) (0.12.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (4.13.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (0.19.2)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (22.1.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (5.10.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair->torch_snippets) (3.10.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_snippets) (2022.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->torch_snippets) (21.3)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->torch_snippets) (21.1.3)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (5.1.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (0.2.0)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 69.9 MB/s \n","\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (2.0.10)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (57.4.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (4.4.2)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->torch_snippets) (4.8.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->torch_snippets) (0.8.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->torch_snippets) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair->torch_snippets) (2.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->torch_snippets) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->torch_snippets) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->torch_snippets) (1.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->torch_snippets) (0.7.0)\n","Collecting Levenshtein==0.20.8\n","  Downloading Levenshtein-0.20.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 72.1 MB/s \n","\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n","  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 71.4 MB/s \n","\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 8.4 MB/s \n","\u001b[?25hBuilding wheels for collected packages: sklearn, typing\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=084e6457a6d2b48f660a22edd9f6720ecfddd2aa9c36b2e7bb1a851d649beb23\n","  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n","  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26323 sha256=9a03a0a956fee0aba9d9c38425015276993b54a5740e4750d30f3ef68e840d34\n","  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n","Successfully built sklearn typing\n","Installing collected packages: rapidfuzz, Levenshtein, jedi, commonmark, xmltodict, typing, sklearn, rich, python-Levenshtein, loguru, jsonlines, fuzzywuzzy, torch-snippets\n","Successfully installed Levenshtein-0.20.8 commonmark-0.9.1 fuzzywuzzy-0.18.0 jedi-0.18.2 jsonlines-3.1.0 loguru-0.6.0 python-Levenshtein-0.20.8 rapidfuzz-2.13.2 rich-12.6.0 sklearn-0.0.post1 torch-snippets-0.499.9 typing-3.7.4.3 xmltodict-0.13.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["typing"]}}},"metadata":{}}]},{"cell_type":"code","source":["def sketch_edges(img):\n","  # RGB Image to Gray Image\n","  rgb2gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n","  # 양방향필터: Gaussian 필터를 양방향으로 적용해 물체선을 완만하게하고 블러링을 어느정도 조절함.\n","  img_gray = cv2.bilateralFilter(rgb2gray,5,50,50)\n","  # Edge detection 테크닉을 통해 스케치 이미지 생성\n","  img_edge = cv2.Canny(img_gray,45,100)\n","  # Invert the binary points\n","  img_edge = cv2.bitwise_not(img_gray)\n","  # Gray Image to RGB Image\n","  img_edges = cv2.cvtColor(img_edge,cv2.COLOR_GRAY2RGB)\n","  return img_edges\n","\n","IMAGE_SIZE = 256"],"metadata":{"id":"E1Qra3npiNNb","executionInfo":{"status":"ok","timestamp":1669385797318,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# width x height x channel 재배치\n","preprocessing = T.Compose([\n","    T.Lambda(lambda x: torch.Tensor(x.copy()).permute(2,0,1).to(device))\n","])\n","# 정규화\n","normalize = lambda x: (x - 127.5)/127.5"],"metadata":{"id":"Ti3Xx-N6kKNw","executionInfo":{"status":"ok","timestamp":1669385797319,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class dataset(Dataset):\n","  def __init__(self,imgs):\n","    self.imgs = imgs # 이미지 입력\n","  def __len__(self): return len(self.imgs) # 총 이미지 수 파악\n","  def __getitem__(self,ix):\n","    fetch = self.imgs[ix]\n","    try:\n","      img = read(fetch,1) \n","    except:\n","      blank = preprocessing(Blank(IMAGE_SIZE,IMAGE_SIZE,3))\n","    img_edges = sketch_edges(img) # 기존 이미지 -> 엣지 이미지로 변환\n","\n","    # 기존 이미지, 엣지 이미지 리사이징&정규화\n","    img, edges = normalize(resize(img,IMAGE_SIZE)), normalize(resize(img_edges, IMAGE_SIZE))\n","\n","    # 엣지 이미지에 원모양 색깔 흩뿌리기\n","    self.__sprinkle_dotted_color_circles_on_edge_img(edges,img)\n","    img, edges = preprocessing(img), preprocessing(edges)\n","    return edges, img\n","  \n","  def __sprinkle_dotted_color_circles_on_edge_img(self,img_src,img_target):\n","    black_coords = self._get_non_white_coordinates(img_target)\n","    for center_y, center_x in black_coords:\n","      self._draw_color_circle_on_src_img(img_src,img_target,center_y, center_x)\n","\n","  def _get_non_white_coordinates(self, img):\n","  \n","    non_white_mask = np.sum(img, axis=-1) < 2.75 #Channel Sum\n","    non_white_y, non_white_x = np.nonzero(non_white_mask) #Returns the x,y coordinates in the form of (array([x0,x1,x2,...,xn]),array([y0,y1,y2,...,yn]))\n","    # randomly sample non-white coordinates\n","    n_non_white = len(non_white_y)\n","    n_color_points = min(n_non_white, 300)\n","    idxs = np.random.choice(n_non_white, n_color_points, replace=False) #Generates \"n_color_points\" random indices in the range of \"n_non_white\"\n","    non_white_coords = list(zip(non_white_y[idxs], non_white_x[idxs]))\n","    return non_white_coords #Final random x,y coordinates\n","\n","  def _draw_color_circle_on_src_img(self, img_src, img_target, center_y, center_x):\n","    assert img_src.shape == img_target.shape, \"Image source and target must have same shape.\"\n","    y0, y1, x0, x1 = self._get_color_point_bbox_coords(center_y, center_x)\n","    color = np.mean(img_target[y0:y1, x0:x1], axis=(0, 1))\n","    img_src[y0:y1, x0:x1] = color #해당 부분 색깔 적용\n","\n","  def _get_color_point_bbox_coords(self, center_y, center_x):\n","    radius = 2\n","    y0 = max(0, center_y-radius+1)\n","    y1 = min(IMAGE_SIZE, center_y+radius)\n","    x0 = max(0, center_x-radius+1)\n","    x1 = min(IMAGE_SIZE, center_x+radius)\n","    return y0, y1, x0, x1 #바운딩박스 좌표값\n","\n","  def choose(self): return self[randint(len(self))]\n"],"metadata":{"id":"cSAifpYckhRu","executionInfo":{"status":"ok","timestamp":1669385797319,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_imgs, val_imgs = train_test_split(Glob('/content/ShoeV2_photo/*.png'), test_size=0.2, random_state=2) #트레이닝 세트/테스트 세트 배분\n","trn_ds, val_ds = dataset(train_imgs), dataset(val_imgs)\n","\n","trn_dl = DataLoader(trn_ds,batch_size=32,shuffle=True)\n","val_dl = DataLoader(val_ds,batch_size=32,shuffle=True)\n","\n","inspect(*next(iter(trn_dl)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":129},"id":"AKF6oKTUqQyQ","executionInfo":{"status":"ok","timestamp":1669385802313,"user_tz":-540,"elapsed":4998,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}},"outputId":"70cfcf16-2f25-4720-fc8f-f8ccf947e5d2"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["══════════════════════════════════════════════════════════════════\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">══════════════════════════════════════════════════════════════════\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tensor  Shape: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m    Min: \u001b[1;36m-1.000\u001b[0m     Max: \u001b[1;36m0.999\u001b[0m      Mean: \u001b[1;36m-0.564\u001b[0m    dtype: \n","torch.float32 @ cu\u001b[1;92mda:0\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Tensor  Shape: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>    Min: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.000</span>     Max: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.999</span>      Mean: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.564</span>    dtype: \n","torch.float32 @ cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["══════════════════════════════════════════════════════════════════\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">══════════════════════════════════════════════════════════════════\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tensor  Shape: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m    Min: \u001b[1;36m-1.000\u001b[0m     Max: \u001b[1;36m1.000\u001b[0m      Mean: \u001b[1;36m0.557\u001b[0m     dtype: \n","torch.float32 @ cu\u001b[1;92mda:0\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Tensor  Shape: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>    Min: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.000</span>     Max: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.000</span>      Mean: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.557</span>     dtype: \n","torch.float32 @ cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["══════════════════════════════════════════════════════════════════\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">══════════════════════════════════════════════════════════════════\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","class UNetDown(nn.Module):\n","    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n","        super(UNetDown, self).__init__()\n","        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n","        if normalize:\n","            layers.append(nn.InstanceNorm2d(out_size))\n","        layers.append(nn.LeakyReLU(0.2))\n","        if dropout:\n","            layers.append(nn.Dropout(dropout))\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","class UNetUp(nn.Module):\n","    def __init__(self, in_size, out_size, dropout=0.0):\n","        super(UNetUp, self).__init__()\n","        layers = [\n","            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n","            nn.InstanceNorm2d(out_size),\n","            nn.ReLU(inplace=True),\n","        ]\n","        if dropout:\n","            layers.append(nn.Dropout(dropout))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x, skip_input):\n","        x = self.model(x)\n","        x = torch.cat((x, skip_input), 1)\n","\n","        return x\n","\n","class GeneratorUNet(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=3):\n","        super(GeneratorUNet, self).__init__()\n","\n","        self.down1 = UNetDown(in_channels, 64, normalize=False)\n","        self.down2 = UNetDown(64, 128)\n","        self.down3 = UNetDown(128, 256)\n","        self.down4 = UNetDown(256, 512, dropout=0.5)\n","        self.down5 = UNetDown(512, 512, dropout=0.5)\n","        self.down6 = UNetDown(512, 512, dropout=0.5)\n","        self.down7 = UNetDown(512, 512, dropout=0.5)\n","        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n","\n","        self.up1 = UNetUp(512, 512, dropout=0.5)\n","        self.up2 = UNetUp(1024, 512, dropout=0.5)\n","        self.up3 = UNetUp(1024, 512, dropout=0.5)\n","        self.up4 = UNetUp(1024, 512, dropout=0.5)\n","        self.up5 = UNetUp(1024, 256)\n","        self.up6 = UNetUp(512, 128)\n","        self.up7 = UNetUp(256, 64)\n","\n","        self.final = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            nn.ZeroPad2d((1, 0, 1, 0)),\n","            nn.Conv2d(128, out_channels, 4, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        d1 = self.down1(x)\n","        d2 = self.down2(d1)\n","        d3 = self.down3(d2)\n","        d4 = self.down4(d3)\n","        d5 = self.down5(d4)\n","        d6 = self.down6(d5)\n","        d7 = self.down7(d6)\n","        d8 = self.down8(d7)\n","        u1 = self.up1(d8, d7)\n","        u2 = self.up2(u1, d6)\n","        u3 = self.up3(u2, d5)\n","        u4 = self.up4(u3, d4)\n","        u5 = self.up5(u4, d3)\n","        u6 = self.up6(u5, d2)\n","        u7 = self.up7(u6, d1)\n","        return self.final(u7)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super(Discriminator, self).__init__()\n","\n","        def discriminator_block(in_filters, out_filters, normalization=True):\n","            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n","            if normalization:\n","                layers.append(nn.InstanceNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *discriminator_block(in_channels * 2, 64, normalization=False),\n","            *discriminator_block(64, 128),\n","            *discriminator_block(128, 256),\n","            *discriminator_block(256, 512),\n","            nn.ZeroPad2d((1, 0, 1, 0)),\n","            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n","        )\n","\n","    def forward(self, img_A, img_B):\n","        img_input = torch.cat((img_A, img_B), 1)\n","        return self.model(img_input)"],"metadata":{"id":"_OuoeH3Zrul5","executionInfo":{"status":"ok","timestamp":1669385802768,"user_tz":-540,"elapsed":459,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["generator = GeneratorUNet().to(device)\n","discriminator = Discriminator().to(device)"],"metadata":{"id":"TvjUpk3JstYU","executionInfo":{"status":"ok","timestamp":1669385803250,"user_tz":-540,"elapsed":488,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def discriminator_train_step(real_src, real_trg, fake_trg):\n","    #discriminator.train()\n","    d_optimizer.zero_grad()\n","\n","    prediction_real = discriminator(real_trg, real_src)\n","    error_real = criterion_GAN(prediction_real, torch.ones(len(real_src), 1, 16, 16).cuda())\n","    error_real.backward()\n","\n","    prediction_fake = discriminator(fake_trg.detach(), real_src)\n","    error_fake = criterion_GAN(prediction_fake, torch.zeros(len(real_src), 1, 16, 16).cuda())\n","    error_fake.backward()\n","\n","    d_optimizer.step()\n","\n","    return error_real + error_fake\n","\n","def generator_train_step(real_src, fake_trg):\n","    #discriminator.train()\n","    g_optimizer.zero_grad()\n","    prediction = discriminator(fake_trg, real_src)\n","\n","    loss_GAN = criterion_GAN(prediction, torch.ones(len(real_src), 1, 16, 16).cuda())\n","    loss_pixel = criterion_pixelwise(fake_trg, real_trg)\n","    loss_G = loss_GAN + lambda_pixel * loss_pixel\n","\n","    loss_G.backward()\n","    g_optimizer.step()\n","    return loss_G\n","\n","denorm = T.Normalize((-1, -1, -1), (2, 2, 2))\n","def sample_prediction():\n","    \"\"\"Saves a generated sample from the validation set\"\"\"\n","    data = next(iter(val_dl))\n","    real_src, real_trg = data\n","    fake_trg = generator(real_src)\n","    img_sample = torch.cat([denorm(real_src[0]), denorm(fake_trg[0]), denorm(real_trg[0])], -1)\n","    img_sample = img_sample.detach().cpu().permute(1,2,0).numpy()\n","    show(img_sample, title='Source::Generated::GroundTruth', sz=12)"],"metadata":{"id":"xj9hnIhQst__","executionInfo":{"status":"ok","timestamp":1669385803252,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["generator = GeneratorUNet().to(device)\n","discriminator = Discriminator().to(device)\n","generator.apply(weights_init_normal)\n","discriminator.apply(weights_init_normal)\n","\n","criterion_GAN = torch.nn.MSELoss()\n","criterion_pixelwise = torch.nn.L1Loss()\n","\n","lambda_pixel = 100\n","g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","val_dl = DataLoader(val_ds, batch_size=1, shuffle=True)\n","\n","epochs = 100\n","log = Report(epochs)\n","\n","for epoch in range(epochs):\n","    N = len(trn_dl)\n","    for bx, batch in enumerate(trn_dl):\n","        real_src, real_trg = batch\n","        fake_trg = generator(real_src)\n","        \n","        errD = discriminator_train_step(real_src, real_trg, fake_trg)\n","        errG = generator_train_step(real_src, fake_trg)\n","        log.record(pos=epoch+(1+bx)/N, errD=errD.item(), errG=errG.item(), end='\\r')\n","\n","    log.report_avgs(epoch+1)\n","    [sample_prediction() for _ in range(2)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1YEYrtffyQ36uKyRyJA_OJ8U9PIwrc2rP"},"id":"j4eAXEIhsv_r","executionInfo":{"status":"ok","timestamp":1669392030434,"user_tz":-540,"elapsed":6227193,"user":{"displayName":"Hoontae Jin","userId":"00356886181411773305"}},"outputId":"f1c45a30-90d4-4a38-9519-6fa406ad4313"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}